{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from xception import Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background, disc, cup, hemorrhage, exudate\n",
    "class_colors = [(0, 0, 0), (255, 0, 0), (255, 0, 255), (0, 255, 0), (255, 255, 0)]\n",
    "\n",
    "def colorize_mask(mask):\n",
    "    colored_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(5):\n",
    "        colored_mask[mask == i] = class_colors[i]\n",
    "    return colored_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separable Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConvoluton(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, padding=0, dilation=1, bias=False):\n",
    "        super(SeparableConvoluton, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size, stride, padding, dilation, groups=in_planes, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(in_planes, out_planes, 1, 1, 0, 1, 1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # reps(몇 번 반복할지, entry flow에서는 1번(입력2), middle flow에서는 2번(입력3)), grow_firs & is_last(블록마다 확인하여 설정)\n",
    "    def __init__(self, in_planes, out_planes, reps, stride=1, dilation=1, start_with_relu=True, grow_first=True, is_last=True):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        # skip connection\n",
    "        if out_planes != in_planes or stride != 1:\n",
    "            self.skip = nn.Conv2d(in_planes, out_planes, 1, stride=stride, bias=False)\n",
    "            self.skipbn = nn.BatchNorm2d(out_planes)\n",
    "        else:\n",
    "            self.skip = None\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        rep = []\n",
    "\n",
    "        # entry flow & middel flow\n",
    "        filters = in_planes # 필터 개수를 맞추기 위함\n",
    "        if grow_first: \n",
    "            rep.append(self.relu) \n",
    "            rep.append(SeparableConvoluton(in_planes, out_planes, 3, stride=1, dilation=dilation)) \n",
    "            rep.append(nn.BatchNorm2d(out_planes))\n",
    "            filters = out_planes\n",
    "        \n",
    "        # 반복\n",
    "        for _ in range(reps-1):\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConvoluton(filters, filters, 3, stride=1, dilation=dilation))\n",
    "            rep.append(nn.BatchNorm2d(filters))\n",
    "        \n",
    "        # entry flow에서 사용\n",
    "        if not grow_first:\n",
    "            rep.append(self.relu)\n",
    "            rep.append(SeparableConvoluton(in_planes, out_planes, 3, stride=1, dilation=dilation))\n",
    "            rep.append(nn.BatchNorm2d(out_planes))\n",
    "        \n",
    "        # relu가 시작이 아닐 경우 (entry flow에서 사용)\n",
    "        if not start_with_relu:\n",
    "            rep = rep[1:]\n",
    "        \n",
    "        if stride != 1: # 각 블록에서 마지막에 stride가 2일 경우\n",
    "            rep.append(SeparableConvoluton(out_planes, out_planes, 3, stride=2))\n",
    "        \n",
    "        if stride == 1 and is_last: # exit flow의 마지막에 사용\n",
    "            rep.append(SeparableConvoluton(out_planes, out_planes, 3, stride=1))\n",
    "\n",
    "        self.rep = nn.Sequential(*rep)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.rep(input)\n",
    "\n",
    "        # skip connection\n",
    "        if self.skip is not None:\n",
    "            skip = self.skip(input)\n",
    "            skip = self.skipbn(skip)\n",
    "        else:\n",
    "            skip = input\n",
    "        \n",
    "        x += skip\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified Aligned Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module):\n",
    "    def __init__(self, in_planes=3, os=16):\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        if os == 16:\n",
    "            entry_block3_stride = 2\n",
    "            middle_block_rate = 1\n",
    "            exit_block_rates = (1, 2)\n",
    "        elif os == 8:\n",
    "            entry_block3_stride = 1\n",
    "            middle_block_rate = 2\n",
    "            exit_block_rates = (2, 4)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Entry Flow\n",
    "        self.conv1 = nn.Conv2d(in_planes, 32, 3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.block1 = Block(64, 128, reps=2, stride=2, start_with_relu=False)\n",
    "        self.block2 = Block(128, 256, reps=2, stride=2, start_with_relu=True, grow_first=True)\n",
    "        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, start_with_relu=True, grow_first=True, is_last=True)\n",
    "\n",
    "        # Middle Flow\n",
    "        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_rate, start_with_relu=True, grow_first=True)\n",
    "\n",
    "        # Exit Flow\n",
    "        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_rates[0], start_with_relu=True, grow_first=False, is_last=True)\n",
    "        \n",
    "        self.conv3 = SeparableConvoluton(1024, 1536, 3, stride=1, dilation=exit_block_rates[1])\n",
    "        self.bn3 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv4 = SeparableConvoluton(1536, 1536, 3, stride=1, dilation=exit_block_rates[1])\n",
    "        self.bn4 = nn.BatchNorm2d(1536)\n",
    "\n",
    "        self.conv5 = SeparableConvoluton(1536, 2048, 3, stride=1, dilation=exit_block_rates[1])\n",
    "        self.bn5 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        # Init weights\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entry Flow\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        # low_level_features: block1의 결과값으로, deeplabv3+ 구조에서 사용\n",
    "        low_level_features = x\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        # Middle Flow\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = self.block9(x)\n",
    "        x = self.block10(x)\n",
    "        x = self.block11(x)\n",
    "        x = self.block12(x)\n",
    "        x = self.block13(x)\n",
    "        x = self.block14(x)\n",
    "        x = self.block15(x)\n",
    "        x = self.block16(x)\n",
    "        x = self.block17(x)\n",
    "        x = self.block18(x)\n",
    "        x = self.block19(x)\n",
    "\n",
    "        # Exit Flow\n",
    "        x = self.block20(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x, low_level_features\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            # modules에서 각 레이어가 Conv2d이면 weight를 초기화, BatchNorm2d이면 weight를 1로, bias를 0으로 초기화\n",
    "            if isinstance(m, nn.Conv2d): # isinstance(확인하고자 하는 데이터 값, 확인하고자 하는 데이터 타입), 서로 타입이 같으면 True, 아니면 False\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.kaiming_normal_(m.weight) # input tensor가 He초기값으로 N(0,std^2)의 정규분포를 갖는다.\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeplabv3plus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, rate):\n",
    "        super(ASPP, self).__init__()\n",
    "        if rate == 1:\n",
    "            kernel_size = 1\n",
    "            padding = 0\n",
    "        else:\n",
    "            kernel_size = 3\n",
    "            padding = rate\n",
    "        \n",
    "        self.atrous_convolution == nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=1, padding=padding, dilation=rate, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.__init_weight()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.atrous_convolution(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archictecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabv3plus(nn.Module):\n",
    "    def __init__(self, n_input_channels=3, n_classes=4, os=16, _print=True):\n",
    "        if _print:\n",
    "            print(\"Constructing DeepLabv3+ model...\")\n",
    "            print(f\"Number of input channels: {n_input_channels}\")\n",
    "            print(f\"Output stride: {os}\")\n",
    "            print(f\"Number of classes: {n_classes}\")\n",
    "        super(DeepLabv3plus, self).__init__()\n",
    "\n",
    "        # Atrous Convolution\n",
    "        self.xception_features = Xception(n_input_channels, os) # Xception 객체 생성\n",
    "        # ASPP # output stride가 16일 경우, rate는 1, 6, 12, 18 / output stride가 8일 경우, rate는 1, 12, 24, 36 \n",
    "        if os == 16:\n",
    "            rates = [1, 6, 12, 18]\n",
    "        elif os == 8:\n",
    "            rates = [1, 12, 24, 36]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # aspp의 rate별로 atrous convolution을 진행\n",
    "        self.aspp1 = ASPP(2048, 256, rate=rates[0])\n",
    "        self.aspp2 = ASPP(2048, 256, rate=rates[1])\n",
    "        self.aspp3 = ASPP(2048, 256, rate=rates[2])\n",
    "        self.aspp4 = ASPP(2048, 256, rate=rates[3])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # aspp와 함께 사용할 global average pooling\n",
    "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             nn.Conv2d(2048, 256, 1, stride=1, bias=False),\n",
    "                                             nn.BatchNorm2d(256),\n",
    "                                             nn.ReLU())\n",
    "        \n",
    "        # aspp의 결과물을 concat한 후 적용, 1x1 convolution을 통해 256개로 줄임\n",
    "        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # adopt [1x1, 48] for channel reduction\n",
    "        # low level feature를 1x1 convolution을 통해 48개로 줄임\n",
    "        self.conv2 = nn.Conv2d(256, 48, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(48)\n",
    "\n",
    "        # 마지막 1x1 convolution을 통해 class 개수만큼 output을 생성\n",
    "        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, 3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(256),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(256, 256, 3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(256),\n",
    "                                       nn.ReLU(),\n",
    "                                        nn.Conv2d(256, n_classes, 1, stride=1))\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.xception_features(input) # xception의 결과값과 low level feature를 받음\n",
    "        x1 = self.aspp1(x) # aspp 1~4를 통과\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x) # aspp와 함께 global average pooling을 통과\n",
    "        x5 = F.upsample(x5, size=x4.size()[2:], mode='bilinear', align_corners=True) # x5의 크기를 x4와 같게 조정\n",
    "\n",
    "        x = torch.concat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        x = self.conv1(x) # aspp의 결과물을 concat한 후 1x1 convolution을 통과\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.upsample(x, size=(int(math.ceil(input.size()[-2]/4)),\n",
    "                                int(math.ceil(input.size()[-1]/4))),\n",
    "                                mode='bilinear', align_corners=True) # x upsample\n",
    "        \n",
    "        low_level_features = self.conv2(low_level_features) # low level feature를 1x1 convolution을 통과\n",
    "        low_level_features = self.bn2(low_level_features)\n",
    "        low_level_features = self.relu(low_level_features)\n",
    "\n",
    "        x = torch.concat((x, low_level_features), dim=1) # x와 low level feature concat\n",
    "        x = self.last_conv(x) # 마지막 1x1 convolution을 통과\n",
    "        x = F.upsample(x, size=input.size()[2:], mode='bilinear', align_corners=True) # x의 크기를 input과 같게 조정\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = DeepLabv3plus(n_input_channe=3, n_classes=4, os=16, _print=True)\n",
    "    model.eval()\n",
    "    image = torch.randn(1, 3, 512, 512)\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(image)\n",
    "    print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None, target_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.mask_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_list = sorted(os.listdir(self.img_dir))\n",
    "        image = cv2.imread(os.path.join(self.img_dir, img_list[idx]))\n",
    "        image = self.preprocessing(image)\n",
    "        image = image / 255\n",
    "        \n",
    "        mask_list = sorted(os.listdir(self.mask_dir))\n",
    "        mask = cv2.imread(os.path.join(self.mask_dir, mask_list[idx]), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "    def preprocessing(self, image):\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        image[:, :, 0] = clahe.apply(image[:, :, 0])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_LAB2RGB)\n",
    "        image = cv2.resize(image, (512, 512))\n",
    "        return image\n",
    "    \n",
    "train_dataset = CustomImageDataset(\n",
    "    img_dir='/home/visuworks2019/Desktop/loocus/jaekyung/LOOCUS-FUNDUS-AI/dataset/train/images',\n",
    "    mask_dir='/home/visuworks2019/Desktop/loocus/jaekyung/LOOCUS-FUNDUS-AI/dataset/train/masks',\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_dataset = CustomImageDataset(\n",
    "    img_dir='/home/visuworks2019/Desktop/loocus/jaekyung/LOOCUS-FUNDUS-AI/dataset/test/images',\n",
    "    mask_dir='/home/visuworks2019/Desktop/loocus/jaekyung/LOOCUS-FUNDUS-AI/dataset/test/masks',\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLabv3plus(n_input_channels=3, n_classes=5, os=16, _print=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 150\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print('Start training..')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, masks in tqdm(train_dataloader):\n",
    "        images = images.to(device).float()\n",
    "        masks = masks.to(device).long()\n",
    "        outputs = model(images)['out']\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_dataloader:\n",
    "            images = images.to(device).float()\n",
    "            masks = masks.to(device).long()\n",
    "            outputs = model(images)['out']\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    early_stopping = EarlyStopping(val_loss, model, patience=7, verbose=True, path='../models/checkpoint_{day}')\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_dataloader)}, Val Loss: {val_loss/len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
