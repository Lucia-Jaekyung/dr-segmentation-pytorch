{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deeplabv3 - tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 20\n",
    "DATA_DIR = 'data'\n",
    "NUM_TRAIN_IMAGES = 1000\n",
    "NUM_VAL_IMAGES = 50\n",
    "\n",
    "def read_image(image_path, mask=False):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    if mask:\n",
    "        image = tf.image.decode_png(image, channels=1)\n",
    "        image.set_shape([None, None, 1])\n",
    "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "    else:\n",
    "        image = tf.image.decode_png(image, channels=3)\n",
    "        image.set_shape([None, None, 3])\n",
    "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
    "        image = image / 127.5 - 1\n",
    "    return image\n",
    "\n",
    "def load_data(image_list, mask_list):\n",
    "    image = read_image(image_list)\n",
    "    mask = read_image(mask_list, mask=True)\n",
    "    return image, mask\n",
    "\n",
    "def data_generator(image_list, mask_list):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "# train_dataset = data_generator(train_images, train_masks)\n",
    "# val_dataset = data_generator(val_images, val_masks)\n",
    "# print('Train Dataset:', train_dataset)\n",
    "# print('Validation Dataset:', val_dataset)\n",
    "\n",
    "def convolution_block(block_input, num_filters=256, kernel_size=3,\n",
    "                      dilation_rate=1, padding='same', use_bias=False):\n",
    "    x = layers.Conv2D(num_filters, kernel_size, dilation_rate=dilation_rate,\n",
    "                      padding='same', use_bias=use_bias, kernel_initializer=keras.initializers.HeNormal)(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(size=(dims[-3] // x.shape[1], dims[-2]),\n",
    "                                   interpolation='bilinear')(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate()([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def Deeplabv3(image_size, num_classes):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    resnet50 = keras.applications.ResNet50(weights='imagenet', include_top=False, input_tensor=model_input)\n",
    "\n",
    "    x = resnet50.get_layer('conv4_block6_2_relu').output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "    input_a = layers.UpSampling2D(size=(image_size // 4 // x.shape[1],\n",
    "                                        image_size // 4 // x.shape[2]),\n",
    "                                  interpolation='bilinear')(x)\n",
    "    input_b = resnet50.get_layer('conv2_block3_relu').output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate()([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(size=(image_size // x.shape[1],\n",
    "                                  image_size // x.shape[2]),\n",
    "                            interpolation='bilinear')(x)\n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=1, padding='same')(x)\n",
    "    return keras.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# model = Deeplabv3(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "# model.summary()\n",
    "\n",
    "'''\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=loss, metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(loss, color='blue', label='train_loss')\n",
    "ax1.plot(val_loss, color='red', label='val_loss')\n",
    "ax1.set_title('Train and Validation Loss')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 1)\n",
    "ax2.plot(loss, color='blue', label='train_accuracy')\n",
    "ax2.plot(val_loss, color='red', label='val_accuracy')\n",
    "ax2.set_title('Train and Validation Accuracy')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "colormap = loadmat()['colormap']\n",
    "colormap = colormap * 100\n",
    "colormap = colormap.astype(np.uint8)\n",
    "'''\n",
    "\n",
    "def infer(model, image_tensor):\n",
    "    predictions = model.predict(np.expand_dims((image_tensor), axis=0))\n",
    "    predictions = np.squeeze(predictions)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    return predictions\n",
    "\n",
    "def decode_segmentation_mask(mask, colormap, n_classes):\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    for i in range(0, n_classes):\n",
    "        idx = mask == i\n",
    "        r[idx] = colormap[i, 0]\n",
    "        g[idx] = colormap[i, 1]\n",
    "        b[idx] = colormap[i, 2]\n",
    "    rgb = np.stack([r, g, b], axis=2)\n",
    "    return rgb\n",
    "\n",
    "def get_overlay(image, colored_mask):\n",
    "    image = tf.keras.preprocessing.image.array_to_img(image)\n",
    "    image = np.array(image).astype(np.uint8)\n",
    "    overlay = cv2.addWeighted(image, 0.35, colored_mask, 0.65, 0)\n",
    "    return overlay\n",
    "\n",
    "def plot_samples_matplotlib(display_list, figsize=(5, 3)):\n",
    "    _, axes = plt.subplots(nrows=1, ncols=len(display_list), figsize=figsize)\n",
    "    for i in range(len(display_list)):\n",
    "        if display_list[i].shape[-1] == 3:\n",
    "            axes[i].imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        else:\n",
    "            axes[i].imshow(display_list[i])\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(image_list, colormap, model):\n",
    "    for image_file in image_list:\n",
    "        image_tensor = read_image(image_file)\n",
    "        prediction_mask = infer(image_tensor=image_tensor, model=model)\n",
    "        prediction_colormap = decode_segmentation_mask(prediction_mask, colormap, NUM_CLASSES)\n",
    "        overlay = get_overlay(image_tensor, prediction_colormap)\n",
    "        plot_samples_matplotlib([image_tensor, overlay, prediction_colormap], figsize=(15, 5))\n",
    "\n",
    "# plot_predictions(image_list=val_images[:4], colormap=colormap, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deeplabv3 - pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 20\n",
    "DATA_DIR = 'data'\n",
    "NUM_TRAIN_IMAGES = 1000\n",
    "NUM_VAL_IMAGES = 50\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, mask_list, transform=None):\n",
    "        self.image_list = image_list\n",
    "        self.mask_list = mask_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = read_image(self.image_list[idx])\n",
    "        mask = read_image(self.mask_list[idx], mask=True)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def convolution_block(block_input, num_filters=256, kernel_size=3,\n",
    "                      dilation_rate=1, padding=1, use_bias=False):\n",
    "    conv_layer = nn.Conv2d(block_input.shape[1], num_filters, kernel_size,\n",
    "                           dilation=dilation_rate, padding=padding, bias=use_bias)\n",
    "    x = conv_layer(block_input)\n",
    "    x = nn.BatchNorm2d(num_filters)(x)\n",
    "    return F.relu(x)\n",
    "\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = F.avg_pool2d(dspp_input, kernel_size=(dims[-2], dims[-1]))\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = F.interpolate(x, size=(dims[-2], dims[-1]), mode='bilinear', align_corners=False)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = torch.cat([out_pool, out_1, out_6, out_12, out_18], dim=1)\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "\n",
    "class Deeplabv3(nn.Module):\n",
    "    def __init__(self, image_size, num_classes):\n",
    "        super(Deeplabv3, self).__init__()\n",
    "        resnet50 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "        self.resnet50 = nn.Sequential(*list(resnet50.children())[:-2])\n",
    "\n",
    "        self.dspp = DilatedSpatialPyramidPooling\n",
    "        self.conv1x1_a = nn.Conv2d(2048, num_classes, kernel_size=1)\n",
    "        self.conv1x1_b = nn.Conv2d(256, 48, kernel_size=1)\n",
    "        self.conv_final = nn.Conv2d(num_classes + 48, num_classes, kernel_size=1)\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        x = self.dspp(x)\n",
    "        input_a = self.up1(x)\n",
    "        input_b = self.conv1x1_b(self.resnet50[4].output)\n",
    "        input_b = self.up2(input_b)\n",
    "\n",
    "        x = torch.cat([input_a, input_b], dim=1)\n",
    "        x = convolution_block(x)\n",
    "        x = convolution_block(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.conv_final(x)\n",
    "        return x\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_images, train_masks, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_images, val_masks, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = Deeplabv3(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "\n",
    "# Define training loop, optimizer, loss function, etc.\n",
    "\n",
    "def infer(model, image_tensor):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = model(image_tensor)\n",
    "        predictions = torch.argmax(predictions, dim=1)\n",
    "    return predictions.numpy()\n",
    "\n",
    "# Remaining code for plotting, visualization, and training loop goes here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dr_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
